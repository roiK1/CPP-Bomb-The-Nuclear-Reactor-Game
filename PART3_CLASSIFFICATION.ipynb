{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***עבור עמודות לבל וקוושטין בלבד - לכל השורות יצור קובץ בפורמט סי אס וי***\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "UjLzSyfik-zZ"
      },
      "id": "UjLzSyfik-zZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- שלב 1: קריאת קובץ ה-XLS ---\n",
        "# import pandas as pd\n",
        "\n",
        "# DATA_PATH = \"/content/train.xls\"\n",
        "# # קרא את קובץ ה-XLS שנמצא באותה תקייה של הקולאב\n",
        "# df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# # --- שלב 2: חילוץ רק שתי עמודות ---\n",
        "# filtered_df = df[[\"question\", \"level\"]]\n",
        "\n",
        "# # --- שלב 2: מחיקת כפילויות ---\n",
        "# filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# # --- שלב 3: שמירה ל-CSV ---\n",
        "# filtered_df.to_csv(\"train_filtered.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# # הדפסה לווידוא\n",
        "# filtered_df.head()\n"
      ],
      "metadata": {
        "id": "Q63ueADmjkXE"
      },
      "id": "Q63ueADmjkXE",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "אם מוסיפים את ה סי אס וי ידני אפשר להתחיל מפה"
      ],
      "metadata": {
        "id": "NKUr0jr7p-jw"
      },
      "id": "NKUr0jr7p-jw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 3 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***טעינת הקובץ - מחיקת כפיליות מ סי אס וי***"
      ],
      "metadata": {
        "id": "RjQoH_IxExFl"
      },
      "id": "RjQoH_IxExFl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- שלב 1: טעינת הקובץ ---\n",
        "filtered_df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# --- שלב 2: מחיקת כפילויות ---\n",
        "filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")"
      ],
      "metadata": {
        "id": "fyfx1scUEqzU"
      },
      "id": "fyfx1scUEqzU",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***א-1***"
      ],
      "metadata": {
        "id": "f_aW02ESJLOo"
      },
      "id": "f_aW02ESJLOo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ייבוא ספריות ובדיקה ראשונית של התוויות***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gm7encWEu_u"
      },
      "id": "5gm7encWEu_u"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check that the expected columns exist and inspect the data\n",
        "print(filtered_df.columns)\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Show global label distribution for 'level'\n",
        "print(\"\\nGlobal distribution of 'level':\")\n",
        "print(filtered_df[\"level\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxymC_hSmq8K",
        "outputId": "c065c886-14a4-47b2-9214-010b2115e606"
      },
      "id": "sxymC_hSmq8K",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['question', 'level'], dtype='object')\n",
            "                                            question   level\n",
            "0  Which magazine was started first Arthur's Maga...  medium\n",
            "1  The Oberoi family is part of a hotel company t...  medium\n",
            "2  Musician and satirist Allie Goertz wrote a son...    hard\n",
            "3    What nationality was James Henry Miller's wife?  medium\n",
            "4  Cadmium Chloride is slightly soluble in this c...  medium\n",
            "\n",
            "Global distribution of 'level':\n",
            "level\n",
            "medium    0.628149\n",
            "easy      0.198688\n",
            "hard      0.173162\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***חלוקה מאוזנת ל־train / validation / test (עם stratify)***"
      ],
      "metadata": {
        "id": "2x9zwKKfpeX-"
      },
      "id": "2x9zwKKfpeX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split proportions\n",
        "TEST_SIZE = 0.15      # 15% of total data for test\n",
        "VAL_SIZE = 0.15       # 15% of total data for validation\n",
        "RANDOM_STATE = 42     # For reproducibility\n",
        "\n",
        "# Compute validation size relative to the remaining data after test split\n",
        "val_size_relative = VAL_SIZE / (1 - TEST_SIZE)  # e.g., 0.15 / 0.85\n",
        "\n",
        "print(\"Relative validation size (from train_val):\", val_size_relative)\n",
        "\n",
        "# Step 1: Split into train_val and test with stratification on 'level'\n",
        "train_val_df, test_df = train_test_split(\n",
        "    filtered_df,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=filtered_df[\"level\"],\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Step 2: Split train_val into train and validation with stratification on 'level'\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=val_size_relative,\n",
        "    stratify=train_val_df[\"level\"],\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Finished stratified split into train / validation / test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m176qvlxpdZJ",
        "outputId": "2dc51e91-dbc9-4b88-90e2-c0f4eca93f2a"
      },
      "id": "m176qvlxpdZJ",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative validation size (from train_val): 0.17647058823529413\n",
            "Finished stratified split into train / validation / test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***בדיקה שהחלוקה מאוזנת (stratified) ושיש לנו את היחסים הרצויים***"
      ],
      "metadata": {
        "id": "rpBACoIoplWC"
      },
      "id": "rpBACoIoplWC"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_split_info(df, name):\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(\"Number of rows:\", len(df))\n",
        "    print(\"Label distribution for 'level':\")\n",
        "    print(df[\"level\"].value_counts(normalize=True))\n",
        "\n",
        "print(\"Total rows in original filtered_df:\", len(filtered_df))\n",
        "\n",
        "print_split_info(train_df, \"Train set\")\n",
        "print_split_info(val_df, \"Validation set\")\n",
        "print_split_info(test_df, \"Test set\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pev31-BLplxP",
        "outputId": "186f29b0-3c21-4f02-d027-428aa2062b7a"
      },
      "id": "pev31-BLplxP",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in original filtered_df: 90418\n",
            "\n",
            "Train set:\n",
            "Number of rows: 63292\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628152\n",
            "easy      0.198682\n",
            "hard      0.173166\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Validation set:\n",
            "Number of rows: 13563\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628180\n",
            "easy      0.198702\n",
            "hard      0.173118\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test set:\n",
            "Number of rows: 13563\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628106\n",
            "easy      0.198702\n",
            "hard      0.173192\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **א-2**"
      ],
      "metadata": {
        "id": "KtfJcpMAJofI"
      },
      "id": "KtfJcpMAJofI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text preprocessing (tokenization + lemmatization)***"
      ],
      "metadata": {
        "id": "zeHPEQzQJsf6"
      },
      "id": "zeHPEQzQJsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download required NLTK resources (only once)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words(\"english\"))\n",
        "BE_FORMS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"}\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    \"\"\"\n",
        "    Map POS tag from nltk.pos_tag to a WordNet POS tag.\n",
        "    This helps the lemmatizer pick the correct base form.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    if tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# Regex patterns for cleaning\n",
        "url_email_handle_re = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)\", re.IGNORECASE)\n",
        "digits_re = re.compile(r\"\\d+\")            # digits -> _number\n",
        "non_letter_re = re.compile(r\"[^a-z_ ]+\")  # after lowercase, keep only a-z, space, underscore\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full preprocessing for a single text value:\n",
        "    - Remove URLs, emails, and @handles/#hashtags\n",
        "    - Tokenize\n",
        "    - POS tagging\n",
        "    - Lemmatization with POS\n",
        "    - Normalize 'be' verb forms\n",
        "    - Replace digits with '_number'\n",
        "    - Remove non-letter characters (keep a-z, space, underscore)\n",
        "    - Remove stopwords\n",
        "    - Lowercase\n",
        "    Returns a cleaned string with space-separated tokens.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    t = text\n",
        "\n",
        "    # Remove URLs, emails, handles, hashtags\n",
        "    t = url_email_handle_re.sub(\" \", t)\n",
        "\n",
        "    # Tokenize and POS-tag on original text\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # Normalize 'be' forms early\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "\n",
        "        # Map tag to WordNet POS and lemmatize\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemma = lemmatizer.lemmatize(tok, wn_pos)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    # Lowercase\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    # Replace digits inside tokens with '_number'\n",
        "    lemmas = [digits_re.sub(\"_number\", w) for w in lemmas]\n",
        "\n",
        "    # Keep only a-z / underscore / spaces, and clean token by token\n",
        "    clean_lemmas = []\n",
        "    for w in lemmas:\n",
        "        w2 = non_letter_re.sub(\" \", w).strip()\n",
        "        if not w2:\n",
        "            continue\n",
        "        # If cleaning produced multiple parts, split them\n",
        "        for part in w2.split():\n",
        "            clean_lemmas.append(part)\n",
        "\n",
        "    # # Remove stopwords\n",
        "    # clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # Join back into a single string\n",
        "    return \" \".join(clean_lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAKXIEoMcY3",
        "outputId": "9adfe9a4-4128-4575-8883-9750eb8e35ff"
      },
      "id": "toAKXIEoMcY3",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apply preprocessing to train / val / test***"
      ],
      "metadata": {
        "id": "qwOP_ovgRKwO"
      },
      "id": "qwOP_ovgRKwO"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Apply processing to 'question' column in each split\n",
        "train_df[\"clean_text\"] = train_df[\"question\"].apply(process_text_value)\n",
        "val_df[\"clean_text\"]   = val_df[\"question\"].apply(process_text_value)\n",
        "test_df[\"clean_text\"]  = test_df[\"question\"].apply(process_text_value)\n",
        "\n",
        "# Also create a token list from the cleaned string (for Word2Vec etc.)\n",
        "train_df[\"tokens\"] = train_df[\"clean_text\"].apply(lambda s: s.split())\n",
        "val_df[\"tokens\"]   = val_df[\"clean_text\"].apply(lambda s: s.split())\n",
        "test_df[\"tokens\"]  = test_df[\"clean_text\"].apply(lambda s: s.split())\n",
        "\n",
        "train_df[[\"question\", \"clean_text\"]].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "VPjsviBsQxIg",
        "outputId": "faaeef24-5c62-4003-f168-fd4a4ac93e6f"
      },
      "id": "VPjsviBsQxIg",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                question  \\\n",
              "72693  How many studio albums has the band that playe...   \n",
              "24973  When was the team that won the 1981-82 Turkish...   \n",
              "19388  What genre of music do Sonic Reign and Emperor...   \n",
              "67473                       What is Rollkommando Hamann?   \n",
              "62224  Who starred with Jay Mohr in a 1997 American r...   \n",
              "\n",
              "                                              clean_text  \n",
              "72693  how many studio album have the band that play ...  \n",
              "24973  when be the team that win the _number _number ...  \n",
              "19388  what genre of music do sonic reign and emperor...  \n",
              "67473                        what be rollkommando hamann  \n",
              "62224  who star with jay mohr in a _number american r...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45d5aa01-2297-48e8-960e-9d60f60d4fe3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72693</th>\n",
              "      <td>How many studio albums has the band that playe...</td>\n",
              "      <td>how many studio album have the band that play ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24973</th>\n",
              "      <td>When was the team that won the 1981-82 Turkish...</td>\n",
              "      <td>when be the team that win the _number _number ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19388</th>\n",
              "      <td>What genre of music do Sonic Reign and Emperor...</td>\n",
              "      <td>what genre of music do sonic reign and emperor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67473</th>\n",
              "      <td>What is Rollkommando Hamann?</td>\n",
              "      <td>what be rollkommando hamann</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62224</th>\n",
              "      <td>Who starred with Jay Mohr in a 1997 American r...</td>\n",
              "      <td>who star with jay mohr in a _number american r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45d5aa01-2297-48e8-960e-9d60f60d4fe3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45d5aa01-2297-48e8-960e-9d60f60d4fe3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45d5aa01-2297-48e8-960e-9d60f60d4fe3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7f5ab9d4-30a1-46b0-8caf-92bda1a04c2b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f5ab9d4-30a1-46b0-8caf-92bda1a04c2b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7f5ab9d4-30a1-46b0-8caf-92bda1a04c2b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_df[[\\\"question\\\", \\\"clean_text\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"When was the team that won the 1981-82 Turkish First Football League championship founded?\",\n          \"Who starred with Jay Mohr in a 1997 American romantic comedy?\",\n          \"What genre of music do Sonic Reign and Emperor fall under?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"when be the team that win the _number _number turkish first football league championship found\",\n          \"who star with jay mohr in a _number american romantic comedy\",\n          \"what genre of music do sonic reign and emperor fall under\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF representation (document → vector)***"
      ],
      "metadata": {
        "id": "ovO3e28_RrXX"
      },
      "id": "ovO3e28_RrXX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define a TF-IDF vectorizer\n",
        "# You can adjust max_features depending on dataset size\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,   # limit vocabulary size (optional)\n",
        "    ngram_range=(1, 1),   # unigrams only\n",
        ")\n",
        "\n",
        "# Fit on train set and transform all splits\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"clean_text\"])\n",
        "X_val_tfidf   = tfidf_vectorizer.transform(val_df[\"clean_text\"])\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(test_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"X_val_tfidf:  \", X_val_tfidf.shape)\n",
        "print(\"X_test_tfidf: \", X_test_tfidf.shape)\n",
        "print(\"(Num of documents, max_features)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqDPZN-mRvtC",
        "outputId": "4b292ed8-ad71-42ec-81e5-516daf3d1561"
      },
      "id": "tqDPZN-mRvtC",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shapes:\n",
            "X_train_tfidf: (63292, 10000)\n",
            "X_val_tfidf:   (13563, 10000)\n",
            "X_test_tfidf:  (13563, 10000)\n",
            "(Num of documents, max_features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train Word2Vec on tokens (word embeddings)***"
      ],
      "metadata": {
        "id": "AsSG0mtZJrEL"
      },
      "id": "AsSG0mtZJrEL"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Prepare sentences for Word2Vec (list of token lists)\n",
        "train_sentences = train_df[\"tokens\"].tolist()\n",
        "\n",
        "# Train a Word2Vec model on the training set only\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    vector_size=100,   # size of word vectors\n",
        "    window=5,          # context window\n",
        "    min_count=2,       # ignore words with total frequency < 2\n",
        "    workers=4,         # number of CPU threads (Colab usually supports this)\n",
        "    sg=1,              # 1 = skip-gram, 0 = CBOW\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Word2Vec model trained.\")\n",
        "print(\"Vocabulary size:\", len(w2v_model.wv.key_to_index))\n",
        "print(\"Vector size:\", w2v_model.vector_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZm-oaYnR7-j",
        "outputId": "56158fc0-d4d3-4ba4-eed9-92e2a4232754"
      },
      "id": "fZm-oaYnR7-j",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Word2Vec model trained.\n",
            "Vocabulary size: 27117\n",
            "Vector size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build document vectors from Word2Vec (TF-IDF weighted average)***"
      ],
      "metadata": {
        "id": "5ke7Pi-KUlGD"
      },
      "id": "5ke7Pi-KUlGD"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build a dictionary: word -> IDF score, based on the TF-IDF vocabulary\n",
        "idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(),\n",
        "                      tfidf_vectorizer.idf_))\n",
        "\n",
        "def document_vector(tokens, use_tfidf_weight=True):\n",
        "    \"\"\"\n",
        "    Compute a single document vector from word vectors.\n",
        "    By default uses TF-IDF weights as recommended.\n",
        "    - tokens: list of preprocessed, lemmatized tokens\n",
        "    - use_tfidf_weight: if True, weight each word vector by its IDF\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    weights = []\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in w2v_model.wv:\n",
        "            vec = w2v_model.wv[tok]\n",
        "            if use_tfidf_weight:\n",
        "                weight = idf_scores.get(tok, 1.0)\n",
        "            else:\n",
        "                weight = 1.0\n",
        "            vectors.append(vec * weight)\n",
        "            weights.append(weight)\n",
        "\n",
        "    if not vectors:\n",
        "        # If no token has a vector, return a zero vector\n",
        "        return np.zeros(w2v_model.vector_size, dtype=np.float32)\n",
        "\n",
        "    vectors = np.vstack(vectors)\n",
        "    weights = np.array(weights, dtype=np.float32)\n",
        "\n",
        "    # Weighted average: sum(w_i * v_i) / sum(w_i)\n",
        "    return vectors.sum(axis=0) / weights.sum()\n",
        "\n",
        "# Build document-level vectors for each split\n",
        "X_train_w2v = np.vstack(train_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "X_val_w2v   = np.vstack(val_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "X_test_w2v  = np.vstack(test_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "\n",
        "print(\"Word2Vec document matrices shapes:\")\n",
        "print(\"X_train_w2v:\", X_train_w2v.shape)\n",
        "print(\"X_val_w2v:  \", X_val_w2v.shape)\n",
        "print(\"X_test_w2v: \", X_test_w2v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwRZS0yDUudb",
        "outputId": "7f38ab2a-8b80-4974-8ebf-47ed998dab44"
      },
      "id": "ZwRZS0yDUudb",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec document matrices shapes:\n",
            "X_train_w2v: (63292, 100)\n",
            "X_val_w2v:   (13563, 100)\n",
            "X_test_w2v:  (13563, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ב-1-סיווג בינארי***"
      ],
      "metadata": {
        "id": "2HJK_nb4xcES"
      },
      "id": "2HJK_nb4xcES"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Filter to binary classes (easy, hard)***"
      ],
      "metadata": {
        "id": "AdkAzZ-Q11NJ"
      },
      "id": "AdkAzZ-Q11NJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only 'easy' and 'hard' classes\n",
        "binary_train = train_df[train_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "binary_val   = val_df[val_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "binary_test  = test_df[test_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "\n",
        "print(\"Train size:\", len(binary_train))\n",
        "print(\"Validation size:\", len(binary_val))\n",
        "print(\"Test size:\", len(binary_test))\n",
        "\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(binary_train[\"level\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcjvK2Ndxcf8",
        "outputId": "c851c3f5-b2f6-4831-931a-0cb9e87c2745"
      },
      "id": "NcjvK2Ndxcf8",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 23535\n",
            "Validation size: 5043\n",
            "Test size: 5044\n",
            "\n",
            "Train label distribution:\n",
            "level\n",
            "easy    0.534311\n",
            "hard    0.465689\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Encode labels (easy=0, hard=1)***"
      ],
      "metadata": {
        "id": "ZaCcDxS601Pd"
      },
      "id": "ZaCcDxS601Pd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(binary_train[\"level\"])\n",
        "y_val   = le.transform(binary_val[\"level\"])\n",
        "y_test  = le.transform(binary_test[\"level\"])\n",
        "\n",
        "print(\"Label classes:\", le.classes_)  # ['easy' 'hard']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egO2tI2e07eI",
        "outputId": "69ade137-8d33-4ff5-aae4-d8435abb7758"
      },
      "id": "egO2tI2e07eI",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes: ['easy' 'hard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build TF-IDF for the binary subsets***"
      ],
      "metadata": {
        "id": "kkibpNFb08Gx"
      },
      "id": "kkibpNFb08Gx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the same TF-IDF vectorizer that was already fitted on full train_df\n",
        "X_train_tfidf_bin = tfidf_vectorizer.transform(binary_train[\"clean_text\"])\n",
        "X_val_tfidf_bin   = tfidf_vectorizer.transform(binary_val[\"clean_text\"])\n",
        "X_test_tfidf_bin  = tfidf_vectorizer.transform(binary_test[\"clean_text\"])\n",
        "\n",
        "print(\"Binary TF-IDF shapes:\")\n",
        "print(\"X_train_tfidf_bin:\", X_train_tfidf_bin.shape)\n",
        "print(\"X_val_tfidf_bin:  \", X_val_tfidf_bin.shape)\n",
        "print(\"X_test_tfidf_bin: \", X_test_tfidf_bin.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2qL3tCy08wD",
        "outputId": "de41eb05-6343-4491-c5f1-4f480f58fd70"
      },
      "id": "O2qL3tCy08wD",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary TF-IDF shapes:\n",
            "X_train_tfidf_bin: (23535, 10000)\n",
            "X_val_tfidf_bin:   (5043, 10000)\n",
            "X_test_tfidf_bin:  (5044, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build Word2Vec document vectors for the binary subsets***"
      ],
      "metadata": {
        "id": "6JT2DoNx23au"
      },
      "id": "6JT2DoNx23au"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you already have w2v_model and document_vector() defined\n",
        "\n",
        "X_train_w2v_bin = np.vstack(\n",
        "    binary_train[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_val_w2v_bin = np.vstack(\n",
        "    binary_val[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_test_w2v_bin = np.vstack(\n",
        "    binary_test[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "\n",
        "print(\"Binary Word2Vec shapes:\")\n",
        "print(\"X_train_w2v_bin:\", X_train_w2v_bin.shape)\n",
        "print(\"X_val_w2v_bin:  \", X_val_w2v_bin.shape)\n",
        "print(\"X_test_w2v_bin: \", X_test_w2v_bin.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdtdkzqG21nt",
        "outputId": "e4594114-e9ce-4ffb-a7a4-31712d8f6b8b"
      },
      "id": "YdtdkzqG21nt",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Word2Vec shapes:\n",
            "X_train_w2v_bin: (23535, 100)\n",
            "X_val_w2v_bin:   (5043, 100)\n",
            "X_test_w2v_bin:  (5044, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Utility: evaluate model***"
      ],
      "metadata": {
        "id": "wgrwWdZi1LvQ"
      },
      "id": "wgrwWdZi1LvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(model_name, representation_name, y_true, y_pred):\n",
        "    print(f\"\\n=== {model_name} + {representation_name} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "9DI7lMjR1Q07"
      },
      "id": "9DI7lMjR1Q07",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Naive Bayes***"
      ],
      "metadata": {
        "id": "ycZgqE4Q1ZIl"
      },
      "id": "ycZgqE4Q1ZIl"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf_bin, y_train)\n",
        "\n",
        "pred_val = nb_tfidf.predict(X_val_tfidf_bin)\n",
        "\n",
        "evaluate_model(\"Naive Bayes\", \"TF-IDF\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN4QUQm11dqL",
        "outputId": "d474aec7-8382-4433-eefb-3ce8784a0f64"
      },
      "id": "WN4QUQm11dqL",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes + TF-IDF ===\n",
            "Accuracy: 0.6365258774538964\n",
            "F1 Score: 0.5534713763702801\n",
            "Confusion Matrix:\n",
            " [[2074  621]\n",
            " [1212 1136]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Logistic Regression***"
      ],
      "metadata": {
        "id": "W5XLMNuQ1eFx"
      },
      "id": "W5XLMNuQ1eFx"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_tfidf = LogisticRegression(max_iter=2000)\n",
        "lr_tfidf.fit(X_train_tfidf_bin, y_train)\n",
        "\n",
        "pred_val = lr_tfidf.predict(X_val_tfidf_bin)\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", \"TF-IDF\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lSuSL5Q1erE",
        "outputId": "0e156c6a-e829-4811-d724-6a8abb7d4048"
      },
      "id": "6lSuSL5Q1erE",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + TF-IDF ===\n",
            "Accuracy: 0.7083085465000991\n",
            "F1 Score: 0.6860192102454642\n",
            "Confusion Matrix:\n",
            " [[1965  730]\n",
            " [ 741 1607]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Naive Bayes***"
      ],
      "metadata": {
        "id": "BDQ-NH5I1fi_"
      },
      "id": "BDQ-NH5I1fi_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_w2v = GaussianNB()\n",
        "nb_w2v.fit(X_train_w2v_bin, y_train)\n",
        "\n",
        "pred_val = nb_w2v.predict(X_val_w2v_bin)\n",
        "\n",
        "evaluate_model(\"Naive Bayes (Gaussian)\", \"Word2Vec\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHl9DRK-1p3W",
        "outputId": "f23fe0dc-9326-4488-ffcf-aa7704021b73"
      },
      "id": "UHl9DRK-1p3W",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Gaussian) + Word2Vec ===\n",
            "Accuracy: 0.5847709696609161\n",
            "F1 Score: 0.4765\n",
            "Confusion Matrix:\n",
            " [[1996  699]\n",
            " [1395  953]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Logistic Regression***"
      ],
      "metadata": {
        "id": "iMBIEtHy1roq"
      },
      "id": "iMBIEtHy1roq"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_w2v = LogisticRegression(max_iter=2000)\n",
        "lr_w2v.fit(X_train_w2v_bin, y_train)\n",
        "\n",
        "pred_val = lr_w2v.predict(X_val_w2v_bin)\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", \"Word2Vec\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT8d4Wnv1ul2",
        "outputId": "a01cd4a1-5cf2-4994-a493-aaf941e58731"
      },
      "id": "FT8d4Wnv1ul2",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + Word2Vec ===\n",
            "Accuracy: 0.6603212373587151\n",
            "F1 Score: 0.6046618970690053\n",
            "Confusion Matrix:\n",
            " [[2020  675]\n",
            " [1038 1310]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ב-1- סיווג רב מחלקתי כלומר 3***"
      ],
      "metadata": {
        "id": "faKA2iZN5NsH"
      },
      "id": "faKA2iZN5NsH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build multi-class subsets (easy, medium, hard)***"
      ],
      "metadata": {
        "id": "dpfRTFW25cZ5"
      },
      "id": "dpfRTFW25cZ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the three target classes\n",
        "target_levels = [\"easy\", \"medium\", \"hard\"]\n",
        "\n",
        "multi_train = train_df[train_df[\"level\"].isin(target_levels)].copy()\n",
        "multi_val   = val_df[val_df[\"level\"].isin(target_levels)].copy()\n",
        "multi_test  = test_df[test_df[\"level\"].isin(target_levels)].copy()\n",
        "\n",
        "print(\"Train size:\", len(multi_train))\n",
        "print(\"Validation size:\", len(multi_val))\n",
        "print(\"Test size:\", len(multi_test))\n",
        "\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(multi_train[\"level\"].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nUnique levels in all splits:\")\n",
        "print(\"Train:\", multi_train[\"level\"].unique())\n",
        "print(\"Val:  \", multi_val[\"level\"].unique())\n",
        "print(\"Test: \", multi_test[\"level\"].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRO-qxBV5c0J",
        "outputId": "a826f25e-5266-434e-ccc7-84f430f9fe2b"
      },
      "id": "GRO-qxBV5c0J",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 63292\n",
            "Validation size: 13563\n",
            "Test size: 13563\n",
            "\n",
            "Train label distribution:\n",
            "level\n",
            "medium    0.628152\n",
            "easy      0.198682\n",
            "hard      0.173166\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Unique levels in all splits:\n",
            "Train: ['medium' 'hard' 'easy']\n",
            "Val:   ['hard' 'medium' 'easy']\n",
            "Test:  ['medium' 'easy' 'hard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Encode labels (3 classes)***"
      ],
      "metadata": {
        "id": "5hSpOfJ95ocX"
      },
      "id": "5hSpOfJ95ocX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_multi = LabelEncoder()\n",
        "\n",
        "y_train_multi = le_multi.fit_transform(multi_train[\"level\"])\n",
        "y_val_multi   = le_multi.transform(multi_val[\"level\"])\n",
        "y_test_multi  = le_multi.transform(multi_test[\"level\"])\n",
        "\n",
        "print(\"Label classes (order):\", le_multi.classes_)  # expects ['easy' 'hard' 'medium'] or similar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9v-DDlP5owa",
        "outputId": "faeaab18-2296-460d-ac4f-23a327d164d7"
      },
      "id": "u9v-DDlP5owa",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes (order): ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF representation for multi-class***"
      ],
      "metadata": {
        "id": "ukMMIGQQ5uQv"
      },
      "id": "ukMMIGQQ5uQv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform clean_text into TF-IDF vectors using the existing fitted vectorizer\n",
        "X_train_tfidf_multi = tfidf_vectorizer.transform(multi_train[\"clean_text\"])\n",
        "X_val_tfidf_multi   = tfidf_vectorizer.transform(multi_val[\"clean_text\"])\n",
        "X_test_tfidf_multi  = tfidf_vectorizer.transform(multi_test[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF shapes (multi-class):\")\n",
        "print(\"X_train_tfidf_multi:\", X_train_tfidf_multi.shape)\n",
        "print(\"X_val_tfidf_multi:  \", X_val_tfidf_multi.shape)\n",
        "print(\"X_test_tfidf_multi: \", X_test_tfidf_multi.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BykdoaMe5uk1",
        "outputId": "f14cd91c-d30b-4baf-e382-71a31b2bb9fe"
      },
      "id": "BykdoaMe5uk1",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shapes (multi-class):\n",
            "X_train_tfidf_multi: (63292, 10000)\n",
            "X_val_tfidf_multi:   (13563, 10000)\n",
            "X_test_tfidf_multi:  (13563, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec document vectors for multi-class***"
      ],
      "metadata": {
        "id": "2tdJzxda527C"
      },
      "id": "2tdJzxda527C"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build document-level vectors using the existing Word2Vec model\n",
        "X_train_w2v_multi = np.vstack(\n",
        "    multi_train[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_val_w2v_multi = np.vstack(\n",
        "    multi_val[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_test_w2v_multi = np.vstack(\n",
        "    multi_test[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "\n",
        "print(\"Word2Vec document shapes (multi-class):\")\n",
        "print(\"X_train_w2v_multi:\", X_train_w2v_multi.shape)\n",
        "print(\"X_val_w2v_multi:  \", X_val_w2v_multi.shape)\n",
        "print(\"X_test_w2v_multi: \", X_test_w2v_multi.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg9D9Qy553Py",
        "outputId": "f9b7bc46-dcd4-4b66-cd3f-e56a44bfc379"
      },
      "id": "pg9D9Qy553Py",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec document shapes (multi-class):\n",
            "X_train_w2v_multi: (63292, 100)\n",
            "X_val_w2v_multi:   (13563, 100)\n",
            "X_test_w2v_multi:  (13563, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation helper (Accuracy, macro-F1, confusion matrix)***"
      ],
      "metadata": {
        "id": "0aNI705N5-dR"
      },
      "id": "0aNI705N5-dR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_multi(model_name, representation_name, y_true, y_pred, label_encoder):\n",
        "    \"\"\"\n",
        "    Print accuracy, macro F1, and confusion matrix for a multi-class setting.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== {model_name} + {representation_name} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Macro F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"Label order:\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "5Gzkgjdw5_u-"
      },
      "id": "5Gzkgjdw5_u-",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Multinomial Naive Bayes (3 classes)***"
      ],
      "metadata": {
        "id": "RIe_Y8Lz6DFw"
      },
      "id": "RIe_Y8Lz6DFw"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_tfidf_multi = MultinomialNB()\n",
        "nb_tfidf_multi.fit(X_train_tfidf_multi, y_train_multi)\n",
        "\n",
        "pred_val_nb_tfidf = nb_tfidf_multi.predict(X_val_tfidf_multi)\n",
        "\n",
        "evaluate_multi(\"Naive Bayes (Multinomial)\", \"TF-IDF\", y_val_multi, pred_val_nb_tfidf, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVV05-U-6EO6",
        "outputId": "7a7a2464-9f64-46c8-d915-02272132e4a3"
      },
      "id": "IVV05-U-6EO6",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Multinomial) + TF-IDF ===\n",
            "Accuracy: 0.6345203863452039\n",
            "Macro F1: 0.28640088063166885\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[ 117    5 2573]\n",
            " [  13    3 2332]\n",
            " [  26    8 8486]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Logistic Regression (3 classes)***"
      ],
      "metadata": {
        "id": "xeA7FIvb6GOD"
      },
      "id": "xeA7FIvb6GOD"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_tfidf_multi = LogisticRegression(max_iter=2000)\n",
        "lr_tfidf_multi.fit(X_train_tfidf_multi, y_train_multi)\n",
        "\n",
        "pred_val_lr_tfidf = lr_tfidf_multi.predict(X_val_tfidf_multi)\n",
        "\n",
        "evaluate_multi(\"Logistic Regression\", \"TF-IDF\", y_val_multi, pred_val_lr_tfidf, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gls-UQsv6Gmt",
        "outputId": "b406c33a-caeb-43bf-d8b7-5eedcf0497cc"
      },
      "id": "Gls-UQsv6Gmt",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + TF-IDF ===\n",
            "Accuracy: 0.6656344466563444\n",
            "Macro F1: 0.43182842804011773\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[1067   24 1604]\n",
            " [ 151   34 2163]\n",
            " [ 496   97 7927]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Gaussian Naive Bayes (3 classes)***"
      ],
      "metadata": {
        "id": "MMubVXrC6Q6P"
      },
      "id": "MMubVXrC6Q6P"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_w2v_multi = GaussianNB()\n",
        "nb_w2v_multi.fit(X_train_w2v_multi, y_train_multi)\n",
        "\n",
        "pred_val_nb_w2v = nb_w2v_multi.predict(X_val_w2v_multi)\n",
        "\n",
        "evaluate_multi(\"Naive Bayes (Gaussian)\", \"Word2Vec\", y_val_multi, pred_val_nb_w2v, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byh1RqoS6SYy",
        "outputId": "a9996e33-cf23-415e-f503-ac26d79364e7"
      },
      "id": "byh1RqoS6SYy",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Gaussian) + Word2Vec ===\n",
            "Accuracy: 0.46988129469881296\n",
            "Macro F1: 0.385821969230117\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[1400  368  927]\n",
            " [ 835  408 1105]\n",
            " [2668 1287 4565]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Logistic Regression (3 classes)***"
      ],
      "metadata": {
        "id": "T2lZMuV_6Vnp"
      },
      "id": "T2lZMuV_6Vnp"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_w2v_multi = LogisticRegression(max_iter=2000)\n",
        "lr_w2v_multi.fit(X_train_w2v_multi, y_train_multi)\n",
        "\n",
        "pred_val_lr_w2v = lr_w2v_multi.predict(X_val_w2v_multi)\n",
        "\n",
        "evaluate_multi(\"Logistic Regression\", \"Word2Vec\", y_val_multi, pred_val_lr_w2v, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E8l5hE66XVc",
        "outputId": "ab4e3c40-de1b-4b3d-f53f-23336e6274f9"
      },
      "id": "9E8l5hE66XVc",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + Word2Vec ===\n",
            "Accuracy: 0.6348890363488904\n",
            "Macro F1: 0.3234048437580385\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[ 329    0 2366]\n",
            " [  54    0 2294]\n",
            " [ 238    0 8282]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Since our experiments clearly showed that TF-IDF consistently outperforms Word2Vec across all models and evaluation metrics, we decided to discontinue the use of Word2Vec and proceed exclusively with TF-IDF representations in the following stages***"
      ],
      "metadata": {
        "id": "wLHtEXAg0hlU"
      },
      "id": "wLHtEXAg0hlU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***הגדרת פונקצייה לניסויים בהיפר-פרמטרים***"
      ],
      "metadata": {
        "id": "leffEAvJ9v08"
      },
      "id": "leffEAvJ9v08"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate_scores(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute accuracy and macro F1 score.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    return acc, f1\n",
        "\n",
        "\n",
        "def tune_nb_tfidf(X_train, y_train, X_val, y_val, alphas, representation_name=\"TF-IDF\"):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning for Multinomial Naive Bayes on TF-IDF features.\n",
        "    Varies the smoothing parameter 'alpha' and prints validation performance.\n",
        "    Returns a list of results (alpha, accuracy, f1_macro).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    print(f\"\\n=== Naive Bayes (Multinomial) + {representation_name} — alpha sweep ===\")\n",
        "    for a in alphas:\n",
        "        model = MultinomialNB(alpha=a)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        acc, f1 = evaluate_scores(y_val, y_pred)\n",
        "        results.append({\"alpha\": a, \"accuracy\": acc, \"f1_macro\": f1})\n",
        "        print(f\"alpha = {a:>4}  ->  Accuracy = {acc:.4f},  Macro F1 = {f1:.4f}\")\n",
        "    # Print best by F1\n",
        "    best = max(results, key=lambda r: r[\"f1_macro\"])\n",
        "    print(f\"\\nBest alpha by macro F1: {best['alpha']} (Accuracy={best['accuracy']:.4f}, F1={best['f1_macro']:.4f})\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def tune_logistic(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    Cs,\n",
        "    max_iter=1000,\n",
        "    representation_name=\"TF-IDF\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning for Logistic Regression on TF-IDF features.\n",
        "    Varies the regularization strength C and prints validation performance.\n",
        "    Returns a list of results (C, accuracy, f1_macro).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    print(f\"\\n=== Logistic Regression + {representation_name} — C sweep (max_iter={max_iter}) ===\")\n",
        "    for c in Cs:\n",
        "        clf = LogisticRegression(C=c, max_iter=max_iter)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        acc, f1 = evaluate_scores(y_val, y_pred)\n",
        "        results.append({\"C\": c, \"accuracy\": acc, \"f1_macro\": f1})\n",
        "        print(f\"C = {c:>5}  ->  Accuracy = {acc:.4f},  Macro F1 = {f1:.4f}\")\n",
        "    # Print best by F1\n",
        "    best = max(results, key=lambda r: r[\"f1_macro\"])\n",
        "    print(f\"\\nBest C by macro F1: {best['C']} (Accuracy={best['accuracy']:.4f}, F1={best['f1_macro']:.4f})\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "NSL6hdnX910h"
      },
      "id": "NSL6hdnX910h",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **תזכורת:**"
      ],
      "metadata": {
        "id": "krPxzvEhA9OL"
      },
      "id": "krPxzvEhA9OL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✔ Accuracy (דיוק)\n",
        "כמה אחוז מהניבואים של המודל היו נכונים מתוך כלל הדוגמאות.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        "אם המודל ניחש נכון 70% מהפעמים → Accuracy = 0.70\n",
        "\n",
        "**מתי זה טוב?**  \n",
        "כאשר הדאטה מאוזן*\n",
        "(כל המחלקות מופיעות בערך באותה כמות).\n",
        "\n",
        "**החיסרון:**  \n",
        "אם מחלקה אחת מופיעה הרבה יותר – המדד עלול להיות מטעה.\n",
        "\n",
        "---\n",
        "\n",
        "### ✔ F1 Score (מדד F1)\n",
        "מדד שמחבר בין\n",
        " Precision ו־Recall\n",
        "  למדד אחד מאוזן.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        " גבוה = המודל גם מוצא נכון דוגמאות של המחלקה וגם לא טועה הרבה.  \n",
        " נמוך = או שהמודל מפספס הרבה דוגמאות, או שהוא טועה הרבה.\n",
        "\n",
        "**מתי משתמשים בו?**  \n",
        "כאשר חשוב לזהות כל מחלקה בצורה טובה במיוחד,\n",
        "או כאשר יש אי־איזון בין המחלקות.\n",
        "\n",
        "---\n",
        "\n",
        "### ✔ Macro F1 (מדד F1 מאקרו)\n",
        "מחשב את ה\n",
        "F1\n",
        " לכל מחלקה בנפרד, ואז עושה ממוצע פשוט ביניהן.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        "כל מחלקה מקבלת משקל שווה — גם אם יש ממנה מעט דוגמאות.\n",
        "\n",
        "**למה זה חשוב?**  \n",
        "כי בבעיות שבהן חלק מהמחלקות מופיעות מעט ,  \n",
        "Accuracy\n",
        " יכול להטעות,\n",
        "אבל\n",
        "Macro F1\n",
        "מוודא שהמודל מצליח גם על המחלקות הקטנות.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XKxwMcfsBAUN"
      },
      "id": "XKxwMcfsBAUN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ניסויים בהיפר פרמטרים***"
      ],
      "metadata": {
        "id": "dDp0khU2_rSd"
      },
      "id": "dDp0khU2_rSd"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Additional Hyperparameter Experiments (TF-IDF only)\n",
        "# ============================================\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) Naive Bayes + TF-IDF with more alpha values\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "nb_alphas_extended = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "nb_tfidf_results_extended = tune_nb_tfidf(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    alphas=nb_alphas_extended,\n",
        "    representation_name=\"TF-IDF (multi-class) — extended alpha\"\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) Logistic Regression + TF-IDF with extended C values\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_C_extended = [0.01, 0.1, 1.0, 10.0, 50.0, 100.0]\n",
        "lr_tfidf_results_extended = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=lr_C_extended,\n",
        "    max_iter=3000,  # slightly higher, helps convergence\n",
        "    representation_name=\"TF-IDF (multi-class) — extended C\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3) Logistic Regression + TF-IDF — small max_iter test\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_tfidf_small_iter = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=[1.0],\n",
        "    max_iter=200,  # very small to check convergence behavior\n",
        "    representation_name=\"TF-IDF (multi-class) — small max_iter\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4) Logistic Regression + TF-IDF — large max_iter test\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_tfidf_large_iter = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=[1.0],\n",
        "    max_iter=5000,  # large enough to guarantee convergence\n",
        "    representation_name=\"TF-IDF (multi-class) — large max_iter\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FpUkxNY_uvG",
        "outputId": "4152858b-b287-4c48-f66a-6b8dab07abd5"
      },
      "id": "-FpUkxNY_uvG",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Multinomial) + TF-IDF (multi-class) — extended alpha — alpha sweep ===\n",
            "alpha = 0.01  ->  Accuracy = 0.6325,  Macro F1 = 0.3088\n",
            "alpha =  0.1  ->  Accuracy = 0.6330,  Macro F1 = 0.3071\n",
            "alpha =  0.5  ->  Accuracy = 0.6347,  Macro F1 = 0.2962\n",
            "alpha =  1.0  ->  Accuracy = 0.6345,  Macro F1 = 0.2864\n",
            "alpha =  2.0  ->  Accuracy = 0.6328,  Macro F1 = 0.2750\n",
            "\n",
            "Best alpha by macro F1: 0.01 (Accuracy=0.6325, F1=0.3088)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — extended C — C sweep (max_iter=3000) ===\n",
            "C =  0.01  ->  Accuracy = 0.6306,  Macro F1 = 0.2659\n",
            "C =   0.1  ->  Accuracy = 0.6661,  Macro F1 = 0.3947\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "C =  10.0  ->  Accuracy = 0.6293,  Macro F1 = 0.4513\n",
            "C =  50.0  ->  Accuracy = 0.6128,  Macro F1 = 0.4494\n",
            "C = 100.0  ->  Accuracy = 0.6104,  Macro F1 = 0.4519\n",
            "\n",
            "Best C by macro F1: 100.0 (Accuracy=0.6104, F1=0.4519)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — small max_iter — C sweep (max_iter=200) ===\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "\n",
            "Best C by macro F1: 1.0 (Accuracy=0.6656, F1=0.4318)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — large max_iter — C sweep (max_iter=5000) ===\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "\n",
            "Best C by macro F1: 1.0 (Accuracy=0.6656, F1=0.4318)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***עד לפה זה החדש =========================================================================================================================================================================================================***"
      ],
      "metadata": {
        "id": "sIysXP7YjYPj"
      },
      "id": "sIysXP7YjYPj"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}